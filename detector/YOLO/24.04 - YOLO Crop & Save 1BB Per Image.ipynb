{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import time\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from scipy import misc, ndimage\n",
    "from PIL import Image\n",
    "\n",
    "from processing_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_crops_train(cropped_images, path, traindf, test_path = None, augment = False, normalize = False):\n",
    "    pad_col = [0, 0, 0]\n",
    "    saved_imgs = []\n",
    "    cropped_imgs = []\n",
    "    cropped_labels = []\n",
    "    crop_filenames = cropped_images['img_name']\n",
    "    labels = cropped_images['class']\n",
    "    labels_set = ['Type_1', 'Type_2', 'Type_3']\n",
    "    for i in range(len(cropped_images)):\n",
    "        if cropped_images['img_name'][i] in saved_imgs:\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                img = ndimage.imread(cropped_images['filename'][i], mode = 'RGB')\n",
    "                x1, x2, y1, y2 = int(cropped_images['x1'][i]), int(cropped_images['x2'][i]), int(cropped_images['y1'][i]), int(cropped_images['y2'][i])\n",
    "                crop_img = img[y1:y2, x1:x2]\n",
    "                h, w = crop_img.shape[0], crop_img.shape[1]\n",
    "                saved_imgs.append(cropped_images['img_name'][i])\n",
    "            except Exception:\n",
    "                print('Failed for image:', cropped_images['filename'][i])\n",
    "                continue\n",
    "            if h < 30 and w < 30:\n",
    "                continue\n",
    "            else:\n",
    "                if h > w:\n",
    "                    crop_img = np.rot90(crop_img)\n",
    "                if h > size[1] and w > size[0]:\n",
    "                    res_img = cv2.resize(crop_img, size, cv2.INTER_AREA)\n",
    "                    if normalize:\n",
    "                        res_img = normalized(res_img)\n",
    "                    try:\n",
    "                        final = Image.fromarray((res_img).astype(np.uint8))\n",
    "                        final.save(path + '{0}/{1}_{2}'.format(labels[i], str(i), crop_filenames[i]))\n",
    "                        cropped_labels.append(labels[i])\n",
    "                    except KeyError:\n",
    "                        print('Saving failed for image: ', crop_filenames[i])\n",
    "                else:\n",
    "                    res_img = cv2.resize(crop_img, size, cv2.INTER_CUBIC)\n",
    "                    if normalize:\n",
    "                        res_img = normalized(res_img)\n",
    "                    try:\n",
    "                        final = Image.fromarray((res_img).astype(np.uint8))\n",
    "                        final.save(path + '{0}/{1}_{2}'.format(labels[i], str(i), crop_filenames[i]))\n",
    "                        cropped_labels.append(labels[i])\n",
    "                    except KeyError:\n",
    "                        print('Saving failed for image: ', crop_filenames[i])\n",
    "                        \n",
    "    crop_set = set(crop_filenames)\n",
    "    test_set = set(traindf['img_name'])\n",
    "    testsetdiff = list(test_set.difference(crop_set))\n",
    "    print('Number of train data set difference images:', len(testsetdiff))\n",
    "    return \n",
    "\n",
    "def make_crops_test(cropped_images, path, test = False, test_path = None, normalize = False):\n",
    "    pad_col = [0, 0, 0]\n",
    "    saved_imgs = []\n",
    "    testfiles = os.listdir(test_path)\n",
    "    crop_filenames = []\n",
    "    for i in range(len(cropped_images)):\n",
    "        crop_filenames.append(cropped_images['filename'][i].split('/')[-1])\n",
    "    crop_set = set(crop_filenames)\n",
    "    \n",
    "    for i in range(len(cropped_images)):\n",
    "        img = cv2.imread(cropped_images['filename'][i])\n",
    "        path = path\n",
    "        if test:\n",
    "            test_filename = cropped_images['filename'][i].split('/')[-1][:-4]\n",
    "            copy_filename = cropped_images['filename'][i].split('/')[-1]\n",
    "        x1, x2, y1, y2 = int(cropped_images['x1'][i]), int(cropped_images['x2'][i]), int(cropped_images['y1'][i]), int(cropped_images['y2'][i])\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        h, w = crop_img.shape[0], crop_img.shape[1]\n",
    "        if h < 30 and w < 30:\n",
    "            print('Crop {} omitted'.format(cropped_images['filename'][i]))\n",
    "            continue\n",
    "        else:\n",
    "            if h > w:\n",
    "                crop_img = np.rot90(crop_img)\n",
    "            if h > size[1] and w > size[0]:\n",
    "                res_img = cv2.resize(crop_img, size, cv2.INTER_AREA)\n",
    "                if normalize:\n",
    "                    res_img = normalized(res_img)\n",
    "                cv2.imwrite(path + test_filename + '_' + str(i) + '.jpg'  , res_img)\n",
    "            else:\n",
    "                res_img = cv2.resize(crop_img, size, cv2.INTER_CUBIC)\n",
    "                if normalize:\n",
    "                    res_img = normalized(res_img)\n",
    "                cv2.imwrite(path + test_filename + '_' + str(i) + '.jpg' , res_img)\n",
    "    \n",
    "    \n",
    "    def set_diff(normalize = False):\n",
    "        print('\\n', 'Take set difference between raw images and crops')\n",
    "        test_set = set(testfiles)\n",
    "        testsetdiff = list(test_set.difference(crop_set))\n",
    "        print('Number of test data set difference images:', len(testsetdiff))\n",
    "        for i in testsetdiff:\n",
    "            diff_img = cv2.imread(test_path + i)\n",
    "            diff_img_resized = cv2.resize(diff_img, size, cv2.INTER_LINEAR)\n",
    "            if normalize:\n",
    "                diff_img_resized = normalized(diff_img_resized)\n",
    "            cv2.imwrite(path + i[:-4] + '_origtest.jpg', diff_img_resized)\n",
    "        return\n",
    "    \n",
    "    #crops_u = recover_crops()\n",
    "    #crops_u = recover_origtest(False)\n",
    "    set_diff(False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_origpath = '/home/w/Development/darknet/cervix_yolo/train_yolo/'\n",
    "tr_croppath = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Intel_Cervix/data/train_crops/'\n",
    "tr_respath = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Old/NCF/darknet2/darknet/Cervix/train_res/train_res100kvoc.txt'\n",
    "\n",
    "tradd_origpath = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Intel_Cervix/data/full_data_renamed/'\n",
    "tradd_croppath = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Intel_Cervix/data/train_additional_0.25_crops/'\n",
    "tradd_respath = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Old/NCF/darknet2/darknet/Cervix/train_additional_res/res100kvoc_combined_025.txt'\n",
    "\n",
    "te_origpath = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Intel_Cervix/data/test/'\n",
    "te_croppath = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Intel_Cervix/data/test_crops/'\n",
    "te_respath = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Old/NCF/darknet2/darknet/Cervix/test_res/res100kvoc.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testdf = load_test(te_origpath)\n",
    "bb_te, co_te = load_boxes(te_respath, testdf)\n",
    "cropte = crop(bb_te)\n",
    "cropte['img_name'] = cropte['filename'].str[-13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindf = load_train(tr_origpath)\n",
    "bb_tr, co_tr = load_boxes(tr_respath, traindf)\n",
    "croptr = crop(bb_tr, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load folder Type_1 (Index: 0)\n",
      "Failed for image: additional_0983.jpg\n",
      "Load folder Type_2 (Index: 1)\n",
      "Failed for image: additional_3059.jpg\n",
      "Failed for image: additional_1573.jpg\n",
      "Load folder Type_3 (Index: 2)\n",
      "Time it took to load train data: 1008.1881549358368\n",
      "Images loaded from: /media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Intel_Cervix/data/full_data_renamed/ \n",
      " \n",
      "       0     1  2                    0\n",
      "0  4128  3096  3  additional_0092.jpg\n",
      "1  4128  3096  3  additional_0050.jpg\n",
      "2  3264  2448  3  additional_0954.jpg\n",
      "3  4160  3120  3  additional_0372.jpg\n",
      "4  4160  3120  3  additional_0749.jpg \n",
      " \n",
      "\n",
      "[]\n",
      "Bounding Boxes results loaded from: /media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Old/NCF/darknet2/darknet/Cervix/train_additional_res/res100kvoc_combined_025.txt \n",
      " \n",
      "                                             filename      xmin      ymin  \\\n",
      "0  /media/w/1c392724-ecf3-4615-8f3c-79368ec36380/...  0.214765  0.428662   \n",
      "1  /media/w/1c392724-ecf3-4615-8f3c-79368ec36380/...  0.384251  0.583946   \n",
      "2  /media/w/1c392724-ecf3-4615-8f3c-79368ec36380/...  0.154297  0.511757   \n",
      "3  /media/w/1c392724-ecf3-4615-8f3c-79368ec36380/...  0.215714  0.707692   \n",
      "4  /media/w/1c392724-ecf3-4615-8f3c-79368ec36380/...  0.328592  0.449431   \n",
      "\n",
      "       xmax      ymax  height  width             img_name   class  \n",
      "0  0.696367  1.023520    4128   3096  additional_0092.jpg  Type_1  \n",
      "1  0.915948  0.933849    4128   3096  additional_0092.jpg  Type_1  \n",
      "2  0.740956  1.116483    4128   3096  additional_0092.jpg  Type_1  \n",
      "3  0.665500  1.002193    4128   3096  additional_0092.jpg  Type_1  \n",
      "4  0.563486  0.633609    4128   3096  additional_0050.jpg  Type_1   \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "traindf = load_train(tradd_origpath)\n",
    "bb_tr, co_tr = load_boxes(tradd_respath, traindf)\n",
    "croptr = crop(bb_tr, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_crops(croptr, len(croptr), 20)\n",
    "#print_crops(cropte, len(cropte), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p1 = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Intel_Cervix/data/train_crops/'\n",
    "p2 = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Intel_Cervix/data/train_additional_0.25_crops/'\n",
    "p3 = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Intel_Cervix/data/train_crops_nondups/'\n",
    "p4 = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Intel_Cervix/data/trainadd_crops_nondups/'\n",
    "\n",
    "labels_set = ['Type_1', 'Type_2', 'Type_3']\n",
    "\n",
    "make_dirs(p2, labels_set)\n",
    "#make_dirs(p4, labels_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for image: /media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Intel_Cervix/data/full_data_renamed/Type_1/additional_0612.jpg\n",
      "Failed for image: /media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Intel_Cervix/data/full_data_renamed/Type_1/additional_0612.jpg\n",
      "Failed for image: /media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Intel_Cervix/data/full_data_renamed/Type_1/original_0233.jpg\n",
      "Number of train data set difference images: 200\n"
     ]
    }
   ],
   "source": [
    "size = (299, 299)\n",
    "\n",
    "#make_crops_test(cropte, te_croppath, True, te_origpath)\n",
    "#make_crops_train(croptr, tr_croppath, traindf, tr_origpath, augment = False)\n",
    "make_crops_train(croptr, tradd_croppath, traindf, tradd_origpath, augment = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
